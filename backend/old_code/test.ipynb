{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, json, numpy as np, random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spot Details:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'zj8Lq1T8KIC5zwFief15jg',\n",
       " 'alias': 'prince-street-pizza-new-york-2',\n",
       " 'name': 'Prince Street Pizza',\n",
       " 'image_url': 'https://s3-media1.fl.yelpcdn.com/bphoto/Jo9jBP5y6G_bG_g3H31fiw/o.jpg',\n",
       " 'is_closed': False,\n",
       " 'url': 'https://www.yelp.com/biz/prince-street-pizza-new-york-2?adjust_creative=g2Uocg3Kx8gT4IQM5axLiQ&utm_campaign=yelp_api_v3&utm_medium=api_v3_business_search&utm_source=g2Uocg3Kx8gT4IQM5axLiQ',\n",
       " 'transactions': ['delivery', 'pickup'],\n",
       " 'location': {'address1': '27 Prince St',\n",
       "  'address2': None,\n",
       "  'address3': '',\n",
       "  'city': 'New York',\n",
       "  'zip_code': '10012',\n",
       "  'country': 'US',\n",
       "  'state': 'NY',\n",
       "  'display_address': ['27 Prince St', 'New York, NY 10012']},\n",
       " 'phone': '+12129664100',\n",
       " 'display_phone': '(212) 966-4100',\n",
       " 'distance': 1950.7368390479296,\n",
       " 'attributes': {'business_temp_closed': None,\n",
       "  'menu_url': 'https://princestreetpizzanyc.com/menu',\n",
       "  'open24_hours': None,\n",
       "  'waitlist_reservation': None}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'zj8Lq1T8KIC5zwFief15jg',\n",
       " 'review_count': 5291,\n",
       " 'categories': [{'alias': 'pizza', 'title': 'Pizza'},\n",
       "  {'alias': 'italian', 'title': 'Italian'}],\n",
       " 'rating': 4.3,\n",
       " 'coordinates': {'latitude': 40.72308755605564,\n",
       "  'longitude': -73.99453001177575},\n",
       " 'price': '$'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Preferences:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'location_specific': {28384: {'time_viewing': 5.0,\n",
       "    'pressed_share': False,\n",
       "    'pressed_save': True,\n",
       "    'rating': 5.0},\n",
       "   28385: {'swiped_left': False,\n",
       "    'swiped_right': True,\n",
       "    'time_viewing': 3.0,\n",
       "    'pressed_share': False,\n",
       "    'pressed_save': True,\n",
       "    'rating': 4.0}},\n",
       "  'general_preferences': {'price': '$$',\n",
       "   'categories': ['restaurant', 'bar'],\n",
       "   'coordinates': (39.9526, 75.1652)}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spots = json.load(open('../datasets/data.json'))\n",
    "\n",
    "spots = spots['businesses']\n",
    "\n",
    "spots[0]\n",
    "#keys to be kept in the recommendation system dataset:\n",
    "#id, categories, review_count, rating, price, coordinates\n",
    "#The rest of the keys are to be stored in a separate dataset for the user's knowledge\n",
    "#keep id in both datasets to link them together\n",
    "initial_weights = []\n",
    "spot_details = []\n",
    "for spot in spots:\n",
    "    rec={}\n",
    "    use={}\n",
    "    for key in spot:\n",
    "        if key == 'id':\n",
    "            rec[key]=spot[key]\n",
    "            use[key]=spot[key]\n",
    "        else:\n",
    "            if key in ['categories','review_count','rating','price','coordinates']:\n",
    "                rec[key]=spot[key]\n",
    "            else:\n",
    "                use[key]=spot[key]\n",
    "        \n",
    "    initial_weights.append(rec)\n",
    "    spot_details.append(use)\n",
    "\n",
    "#Adding keys with made up random values to test drive integrating features into the recommender system.\n",
    "#New Keys: \n",
    "# time_viewing [How much time the user spent looking at the place] (float)\n",
    "# pressed_details [If the user pressed the details button] (bool)\n",
    "# pressed_share [If they pressed share button] (bool)\n",
    "# pressed_save [If they saved the place] (bool)\n",
    "\n",
    "#user_preferences will be a two layer nesteddictionary with the user id as the key and a dictionary of the user's preferences as the value, \n",
    "# the user preferences is a nested dict where keys are spot ids, and values are spot specific preferences below is a sample of what the \n",
    "#user_preferences dictionary will look like\n",
    "user_preferences = {\n",
    "    1:{#uid\n",
    "        'location_specific':{\n",
    "            28384:{#spot_id\n",
    "                'time_viewing':5.0,'pressed_share':False,'pressed_save':True,\n",
    "                'rating':5.0\n",
    "            },\n",
    "            28385:{#spot_id\n",
    "                'swiped_left':False,'swiped_right':True,'time_viewing':3.0,'pressed_share':False,'pressed_save':True,\n",
    "                'rating':4.0\n",
    "            },\n",
    "        },\n",
    "        'general_preferences':{\n",
    "            'price':'$$',\n",
    "            'categories':['restaurant','bar'],\n",
    "            'coordinates':(39.9526,75.1652)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "print(\"Spot Details:\")\n",
    "display(spot_details[0])\n",
    "print(\"Initial Weights:\")\n",
    "display(initial_weights[0])\n",
    "print(\"User Preferences:\")\n",
    "display(user_preferences)\n",
    "\n",
    "#unique categories\n",
    "# categories = set()\n",
    "# for spot in initial_weights:\n",
    "#     for category in spot['categories']:\n",
    "#         for key,value in category.items():\n",
    "#             if key == 'title':\n",
    "#                 categories.add(value)\n",
    "# list(categories)\n",
    "# initial_weights[0]\n",
    "# temp={}\n",
    "# for entry in initial_weights:\n",
    "#     temp[entry['id']]=entry\n",
    "#     del temp[entry['id']]['id']\n",
    "# initial_weights=temp\n",
    "# temp={}\n",
    "# for entry in spot_details:\n",
    "#     temp[entry['id']]=entry\n",
    "#     del temp[entry['id']]['id']\n",
    "# spot_details=temp\n",
    "# json.dump(initial_weights,open('Updating_datasets/initial_weights.json','w'))\n",
    "# json.dump(spot_details,open('Updating_datasets/spot_details.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           id  review_count  \\\n",
      "0      zj8Lq1T8KIC5zwFief15jg          5291   \n",
      "1      j1S3NUrkB3BVT49n_e76NQ          4805   \n",
      "2      lynQoI3w_pzYfHGeuUU-Qg          1443   \n",
      "3      vk7W3_sQwr7eZbRFsXv6rw          3374   \n",
      "4      X8ZS-dgiMIJvhwf9SaDnjw          2159   \n",
      "...                       ...           ...   \n",
      "12160  UGBwmvJ47rDhDGxdUSBZdg            32   \n",
      "12161  FxswnvW4Fat6dFYcQNaE7Q            75   \n",
      "12162  --V7PYYPxSgXU3EmFkVDXQ            28   \n",
      "12163  _BsM67A1_JqAmgXA3SznKg           317   \n",
      "12164  QERbo2vqq4T9GyIFtb2Ccg           392   \n",
      "\n",
      "                                         categories  rating price   latitude  \\\n",
      "0                               European, Fast Food     4.3     $  40.723088   \n",
      "1      Cafes and Coffee Shops, Breakfast and Brunch     4.5     $  40.752268   \n",
      "2                                             Asian     4.5     $  40.717411   \n",
      "3                      Asian, Bakeries and Desserts     4.3     $  40.717890   \n",
      "4                                  Other, Fast Food     4.5     $  40.717350   \n",
      "...                                             ...     ...   ...        ...   \n",
      "12160                                     Fast Food     4.3     $  40.704769   \n",
      "12161          Asian, Breakfast and Brunch, Seafood     4.5     $  40.740320   \n",
      "12162                                         Other     4.8     $  40.705420   \n",
      "12163                Seafood, Latin American, Other     3.9    $$  40.744740   \n",
      "12164                 Arts and Entertainment, Other     4.4     $  40.699338   \n",
      "\n",
      "       longitude  \n",
      "0     -73.994530  \n",
      "1     -73.991086  \n",
      "2     -73.992100  \n",
      "3     -73.998800  \n",
      "4     -73.994570  \n",
      "...          ...  \n",
      "12160 -73.905899  \n",
      "12161 -73.992230  \n",
      "12162 -74.008639  \n",
      "12163 -73.884389  \n",
      "12164 -74.039324  \n",
      "\n",
      "[12165 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "category_mapping = {\n",
    "    'American': ['American', 'New American', 'Southern', 'Soul Food', 'Cajun/Creole', 'Tex-Mex'],\n",
    "    'Asian': ['Chinese', 'Japanese', 'Korean', 'Thai', 'Vietnamese', 'Indian', 'Pakistani', 'Bangladeshi', 'Taiwanese', 'Filipino', 'Malaysian', 'Indonesian', 'Singaporean', 'Burmese', 'Cambodian', 'Laotian', 'Mongolian', 'Nepalese', 'Sri Lankan', 'Asian Fusion'],\n",
    "    'European': ['Italian', 'French', 'Spanish', 'German', 'Greek', 'British', 'Irish', 'Scottish', 'Polish', 'Russian', 'Ukrainian', 'Hungarian', 'Czech', 'Austrian', 'Belgian', 'Dutch', 'Swiss', 'Scandinavian', 'Portuguese'],\n",
    "    'Latin American': ['Mexican', 'Brazilian', 'Peruvian', 'Argentine', 'Colombian', 'Venezuelan', 'Cuban', 'Puerto Rican', 'Dominican', 'Salvadoran', 'Honduran', 'Nicaraguan', 'Guatemalan', 'Ecuadorian', 'Bolivian', 'Chilean'],\n",
    "    'Middle Eastern': ['Lebanese', 'Turkish', 'Persian/Iranian', 'Israeli', 'Moroccan', 'Egyptian', 'Syrian', 'Armenian', 'Afghan', 'Iraqi', 'Uzbek', 'Georgian'],\n",
    "    'African': ['Ethiopian', 'Nigerian', 'Ghanaian', 'Senegalese', 'South African', 'Eritrean', 'Somali', 'Kenyan', 'Tanzanian', 'Ugandan'],\n",
    "    'Seafood': ['Seafood', 'Sushi Bars', 'Fish & Chips', 'Poke'],\n",
    "    'Fast Food': ['Fast Food', 'Burgers', 'Pizza', 'Sandwiches', 'Hot Dogs', 'Chicken Wings'],\n",
    "    'Vegetarian and Vegan': ['Vegetarian', 'Vegan', 'Raw Food'],\n",
    "    'Breakfast and Brunch': ['Breakfast & Brunch', 'Pancakes', 'Waffles', 'Bagels', 'Donuts'],\n",
    "    'Bakeries and Desserts': ['Bakeries', 'Desserts', 'Ice Cream & Frozen Yogurt', 'Cupcakes', 'Patisserie/Cake Shop', 'Gelato'],\n",
    "    'Cafes and Coffee Shops': ['Cafes', 'Coffee & Tea', 'Bubble Tea'],\n",
    "    'Bars and Pubs': ['Bars', 'Pubs', 'Sports Bars', 'Wine Bars', 'Beer Gardens', 'Cocktail Bars', 'Dive Bars', 'Hookah Bars'],\n",
    "    'Specialty Food': ['Cheese Shops', 'Butcher', 'Farmers Market', 'Specialty Food', 'Organic Stores', 'Health Markets'],\n",
    "    'Food Trucks and Stands': ['Food Trucks', 'Food Stands', 'Street Vendors'],\n",
    "    'Grocery': ['Grocery', 'International Grocery', 'Convenience Stores'],\n",
    "    'Nightlife': ['Nightlife', 'Dance Clubs', 'Karaoke', 'Comedy Clubs', 'Jazz & Blues'],\n",
    "    'Arts and Entertainment': ['Museums', 'Art Galleries', 'Performing Arts', 'Music Venues', 'Theaters', 'Cinema'],\n",
    "    'Outdoor Activities': ['Parks', 'Beaches', 'Hiking', 'Botanical Gardens', 'Playgrounds', 'Dog Parks'],\n",
    "    'Fitness and Sports': ['Gyms', 'Yoga', 'Martial Arts', 'Swimming Pools', 'Tennis', 'Basketball Courts', 'Soccer'],\n",
    "    'Shopping': ['Shopping Centers', 'Clothing', 'Shoes', 'Jewelry', 'Books', 'Electronics', 'Home & Garden'],\n",
    "    'Beauty and Spas': ['Hair Salons', 'Nail Salons', 'Day Spas', 'Massage'],\n",
    "    'Hotels and Accommodation': ['Hotels', 'Hostels', 'Bed & Breakfast'],\n",
    "    'Event Planning and Services': ['Wedding Planning', 'Party & Event Planning', 'Caterers', 'Photographers'],\n",
    "    'Automotive': ['Car Dealers', 'Auto Repair', 'Car Wash', 'Gas Stations'],\n",
    "    'Professional Services': ['Lawyers', 'Accountants', 'Real Estate', 'Insurance'],\n",
    "    'Education': ['Schools', 'Colleges', 'Tutoring', 'Cooking Classes', 'Art Schools'],\n",
    "    'Pets': ['Pet Stores', 'Veterinarians', 'Pet Groomers', 'Dog Walkers'],\n",
    "    'Religious Organizations': ['Churches', 'Mosques', 'Synagogues', 'Temples'],\n",
    "    'Other': []  # Catch-all for categories that don't fit elsewhere\n",
    "}\n",
    "\n",
    "# Create a reverse mapping for easy lookup\n",
    "reverse_category_mapping = {sub_cat: main_cat for main_cat, sub_cats in category_mapping.items() for sub_cat in sub_cats}\n",
    "\n",
    "\n",
    "# Load the data\n",
    "with open('../datasets/initial_weights.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "index=[]\n",
    "vals = []\n",
    "for key,value in data.items():\n",
    "    index.append(key)\n",
    "    vals.append([val for val in value.values()])\n",
    "    \n",
    "initial_weights = pd.DataFrame(vals,columns=[key for key in data[index[0]].keys()],index=index)\n",
    "\n",
    "with open('../datasets/spot_details.json', 'r') as file:\n",
    "    spot_details = json.load(file)\n",
    "index,vals = [],[]\n",
    "# print(spot_details)\n",
    "for key,value in spot_details.items():\n",
    "    index.append(key)\n",
    "    vals.append([val for val in value.values()])\n",
    "# print(len(vals),len(index))\n",
    "\n",
    "spot_details = pd.DataFrame(vals,columns=[key for key in spot_details[index[0]].keys()],index=index)\n",
    "spot_details = spot_details.copy().reset_index()\n",
    "spot_details.rename(columns={'index': 'id'}, inplace=True)\n",
    "user_preferences = {}\n",
    "\n",
    "def preprocess_data(initial_weights):\n",
    "    df = initial_weights.copy().reset_index()\n",
    "    df.rename(columns={'index': 'id'}, inplace=True)\n",
    "    \n",
    "    # Extract categories and map to general categories\n",
    "    df['categories'] = df['categories'].apply(lambda x: [reverse_category_mapping.get(cat['title'], 'Other') for cat in x])\n",
    "    df['categories'] = df['categories'].apply(lambda x: ', '.join(set(x)) if x else 'Other')\n",
    "    \n",
    "    # Handle missing values\n",
    "    df['review_count'] = df['review_count'].fillna(0)\n",
    "    df['rating'] = df['rating'].fillna(0)\n",
    "    df['price'] = df['price'].fillna('$')\n",
    "    df['latitude'] = df['coordinates'].apply(lambda x: x['latitude'] if x and 'latitude' in x else 0)\n",
    "    df['longitude'] = df['coordinates'].apply(lambda x: x['longitude'] if x and 'longitude' in x else 0)\n",
    "    df = df.drop(columns=['coordinates'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df = preprocess_data(initial_weights)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12165, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one time thing, should be saved into the database as read only, unless we expand with more datasets\n",
    "def create_feature_matrix(df):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(df['categories'])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    numerical_features = scaler.fit_transform(df[['review_count', 'rating', 'latitude', 'longitude']].fillna(0))\n",
    "    \n",
    "    price_dummies = pd.get_dummies(df['price'], prefix='price').fillna(0)\n",
    "    \n",
    "    features = np.hstack((tfidf_matrix.toarray(), numerical_features, price_dummies.values))\n",
    "    \n",
    "    return features, tfidf, scaler, tfidf_matrix\n",
    "\n",
    "features, tfidf, coordinate_scaler,tfidf_matrix = create_feature_matrix(df)\n",
    "# item_similarity = cosine_similarity(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['european', 'fast food', 'cafes and coffee shops', 'breakfast and brunch', 'asian']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocess each row to replace commas with a unique separator (like '||') to treat the entire category as one token\n",
    "df['processed_categories'] = df['categories'].apply(lambda x: x.replace(', ', '||'))\n",
    "\n",
    "# Initialize TfidfVectorizer with a custom tokenizer that splits on '||' instead of spaces\n",
    "t = TfidfVectorizer(tokenizer=lambda x: x.split('||'))\n",
    "\n",
    "# Fit and transform the processed categories\n",
    "tfidf_matrix = t.fit_transform(df['processed_categories'])\n",
    "\n",
    "# You can now access the vocabulary and use it as needed\n",
    "print(list(t.vocabulary_.keys())[:5])\n",
    "\n",
    "len(t.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user 2 added successfully\n",
      "New user 3 added successfully\n",
      "New user 4 added successfully\n",
      "New user 5 added successfully\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "#This just reads from the database, no need for an api for this\n",
    "###\n",
    "# Function to get user profile\n",
    "def get_user_profile(user_id, tfidf, coordinate_scaler):\n",
    "    if user_id not in user_preferences:\n",
    "        return np.zeros(features.shape[1])\n",
    "    \n",
    "    user_vector = np.zeros(features.shape[1])\n",
    "    user_data = user_preferences[user_id]\n",
    "    \n",
    "    general_prefs = user_data['general_preferences']\n",
    "    for category in general_prefs['categories']:\n",
    "        mapped_category = reverse_category_mapping.get(category, category)\n",
    "        if mapped_category in tfidf.vocabulary_:\n",
    "            user_vector[tfidf.vocabulary_[mapped_category]] = 1\n",
    "    \n",
    "    price_index = features.shape[1] - 4 + len(general_prefs['price'])\n",
    "    user_vector[price_index] = 1\n",
    "    \n",
    "    user_coords = np.array(general_prefs['coordinates']).reshape(1, -1)\n",
    "    # Create a dummy array with 4 features to match the scaler's expected input\n",
    "    dummy_coords = np.zeros((1, 4))\n",
    "    dummy_coords[0, 2:] = user_coords  # Assuming latitude and longitude are the last two features\n",
    "    scaled_coords = coordinate_scaler.transform(dummy_coords)\n",
    "    user_vector[-6:-4] = scaled_coords[0, 2:] \n",
    "\n",
    "    for spot_id, spot_data in user_data['location_specific'].items():\n",
    "        spot_index = df[df['id'] == spot_id].index\n",
    "        if len(spot_index) > 0:\n",
    "            spot_index = spot_index[0]\n",
    "            \n",
    "            if not spot_data.get('time_viewing', True):\n",
    "                # User pressed details, incorporate positive interactions\n",
    "                interaction_weight = 1.0\n",
    "                \n",
    "                if spot_data.get('pressed_share', False):\n",
    "                    interaction_weight += 0.3\n",
    "                if spot_data.get('pressed_save', False):\n",
    "                    interaction_weight += 0.3\n",
    "                \n",
    "                # Incorporate viewing time\n",
    "                max_viewing_time = 60\n",
    "                viewing_time = min(spot_data.get('time_viewing', 0), max_viewing_time)\n",
    "                time_factor = viewing_time / max_viewing_time\n",
    "                interaction_weight *= (1 + time_factor)\n",
    "                \n",
    "                # Incorporate rating with penalty for low ratings\n",
    "                rating = spot_data.get('rating', 2.5)\n",
    "                if rating < 3:\n",
    "                    # Apply penalty that increases as rating approaches 1\n",
    "                    penalty = 1 - (rating - 1) / 2  # This will be 1 at rating 1, and 0 at rating 3\n",
    "                    interaction_weight *= (1 - penalty * 0.5)  # Adjust the 0.5 to control penalty strength\n",
    "                \n",
    "                # Add to user vector\n",
    "                user_vector += features[spot_index] * interaction_weight\n",
    "            \n",
    "            else:\n",
    "                # User didn't press details, subtract a fraction of the feature vector\n",
    "                user_vector -= features[spot_index] * 0.2\n",
    "    \n",
    "    norm = np.linalg.norm(user_vector)\n",
    "    if norm > 0:\n",
    "        user_vector /= norm\n",
    "    \n",
    "    return user_vector\n",
    "\n",
    "####\n",
    "#create an api for this, read from user and write to database\n",
    "###\n",
    "def add_new_user(user_id, general_preferences,name,pword,age):\n",
    "    if user_id in user_preferences:\n",
    "        raise ValueError(\"User ID already exists\")\n",
    "    \n",
    "    user_preferences[user_id] = {\n",
    "        'general_preferences': general_preferences,\n",
    "        'location_specific': {},\n",
    "        'last_active': datetime.now()\n",
    "    }\n",
    "    user_preferences[user_id]['friends'] = []\n",
    "    user_preferences[user_id]['name'] = name\n",
    "    user_preferences[user_id]['password'] = pword\n",
    "    user_preferences[user_id]['age'] = age\n",
    "    print(f\"New user {user_id} added successfully\")\n",
    "\n",
    "####\n",
    "#create an api for this, read from user and write to database\n",
    "###\n",
    "def update_user_preferences(user_id, new_preferences):\n",
    "    if user_id not in user_preferences:\n",
    "        raise ValueError(\"User ID does not exist\")\n",
    "    \n",
    "    user_preferences[user_id]['general_preferences'].update(new_preferences)\n",
    "    user_preferences[user_id]['last_active'] = datetime.now()\n",
    "    print(f\"Preferences updated for user {user_id}\")\n",
    "\n",
    "####\n",
    "#create an api for this, read from user and write to database\n",
    "###\n",
    "def record_spot_interaction(user_id, spot_id, interaction):\n",
    "    if user_id not in user_preferences:\n",
    "        raise ValueError(\"User ID does not exist\")\n",
    "    \n",
    "    if spot_id not in user_preferences[user_id]['location_specific']:\n",
    "        user_preferences[user_id]['location_specific'][spot_id] = {}\n",
    "    \n",
    "    user_preferences[user_id]['location_specific'][spot_id].update(interaction)\n",
    "    user_preferences[user_id]['last_active'] = datetime.now()\n",
    "    print(f\"Interaction recorded for user {user_id} with spot {spot_id}\")\n",
    "\n",
    "# def get_user_stats(user_id):\n",
    "#     if user_id not in user_preferences:\n",
    "#         raise ValueError(\"User ID does not exist\")\n",
    "    \n",
    "#     user_data = user_preferences[user_id]\n",
    "#     total_interactions = len(user_data['location_specific'])\n",
    "#     likes = sum(1 for spot in user_data['location_specific'].values() if spot.get('pressed_details', False))\n",
    "    \n",
    "#     return {\n",
    "#         'total_interactions': total_interactions,\n",
    "#         'likes': likes,\n",
    "#         'last_active': user_data['last_active']\n",
    "#     }\n",
    "\n",
    "\n",
    "####\n",
    "#create an api for this, read from user and write to database\n",
    "###\n",
    "def update_user_coordinates(user_id, new_coordinates):\n",
    "    if user_id not in user_preferences:\n",
    "        raise ValueError(\"User ID does not exist\")\n",
    "    \n",
    "    user_preferences[user_id]['general_preferences']['coordinates'] = new_coordinates\n",
    "    user_preferences[user_id]['last_active'] = datetime.now()\n",
    "    print(f\"Coordinates updated for user {user_id}\")\n",
    "\n",
    "####\n",
    "#internal function, no api needed, just reads from database\n",
    "###\n",
    "def get_next_items(user_id, n=10):\n",
    "    # Ensure n is even\n",
    "    n = n if n % 2 == 0 else n + 1\n",
    "    \n",
    "    # Get n/2 recommendations based on user profile\n",
    "    user_based_recommendations = user_based_recommend(user_id, n // 2)\n",
    "    \n",
    "    # Get n/2 recommendations based on item similarity to the user-based recommendations\n",
    "    item_based_recommendations = item_based_recommend(user_based_recommendations, n // 2)\n",
    "    \n",
    "    # Combine and shuffle the recommendations\n",
    "    all_recommendations = pd.concat([user_based_recommendations, item_based_recommendations])\n",
    "    all_recommendations.sample(n=len(all_recommendations))\n",
    "    return all_recommendations\n",
    "\n",
    "####\n",
    "#internal function, no api needed, just reads from database\n",
    "### \n",
    "def user_based_recommend(user_id, n):\n",
    "    if user_id not in user_preferences:\n",
    "        # New user: use a fallback method (e.g., popular items)\n",
    "        return popular_items_recommend(n)\n",
    "    \n",
    "    user_profile = get_user_profile(user_id, tfidf, coordinate_scaler)\n",
    "    scores = cosine_similarity([user_profile], features)[0]\n",
    "    \n",
    "    top_indices = scores.argsort()[-n:][::-1]\n",
    "    recommended_ids = df.iloc[top_indices]['id'].tolist()\n",
    "    \n",
    "    return spot_details[spot_details['id'].isin(recommended_ids)][['id', 'name', 'image_url', 'phone']]\n",
    "\n",
    "####\n",
    "#internal function, no api needed, just reads from database\n",
    "###\n",
    "def item_based_recommend(base_items, n):\n",
    "    base_indices = df[df['id'].isin(base_items['id'])].index\n",
    "    \n",
    "    similar_items = set()\n",
    "    for idx in base_indices:\n",
    "        # Get top similar items for each base item\n",
    "        similar_indices = item_similarity[idx].argsort()[-n:][::-1]\n",
    "        similar_items.update(df.iloc[similar_indices]['id'].tolist())\n",
    "    \n",
    "    # Remove base items from similar items\n",
    "    similar_items = list(similar_items - set(base_items['id']))\n",
    "    \n",
    "    # If we don't have enough similar items, pad with popular items\n",
    "    if len(similar_items) < n:\n",
    "        popular = popular_items_recommend(n - len(similar_items))\n",
    "        similar_items.extend(popular['id'].tolist())\n",
    "    \n",
    "    return spot_details[spot_details['id'].isin(similar_items[:n])][['id', 'name', 'image_url', 'phone']]\n",
    "\n",
    "####\n",
    "#internal function, no api needed, just reads from database\n",
    "###\n",
    "def popular_items_recommend(n):\n",
    "    # Recommend based on a combination of rating and review count\n",
    "    scores = df['review_count'] * df['rating'].fillna(0)\n",
    "    top_indices = scores.argsort()[-n:][::-1]\n",
    "    recommended_ids = df.iloc[top_indices]['id'].tolist()\n",
    "    return spot_details[spot_details['id'].isin(recommended_ids)][['id', 'name', 'image_url', 'phone']]\n",
    "\n",
    "####\n",
    "#create an api for this, read from database and display in ui (put to front end)\n",
    "###\n",
    "# Update the get_next_spot function to use get_next_items\n",
    "def get_next_spot(user_id):\n",
    "    recommendations = get_next_items(user_id, n=10)  # Get 10 recommendations\n",
    "    seen_spots = set(user_preferences[user_id]['location_specific'].keys())\n",
    "\n",
    "    #  [] [0] = location     {id:[data]}\n",
    "    \n",
    "    for _, spot in recommendations.iterrows():\n",
    "        if spot['id'] not in seen_spots:\n",
    "            return spot\n",
    "    \n",
    "    return None  # Return None if all recommended spots have been seen\n",
    "\n",
    "####\n",
    "#internal function, no api needed, just reads from database\n",
    "###\n",
    "def get_group_profile(user_ids, tfidf, coordinate_scaler):\n",
    "    group_vector = np.zeros(features.shape[1])\n",
    "    for user_id in user_ids:\n",
    "        user_vector = get_user_profile(user_id, tfidf, coordinate_scaler)\n",
    "        group_vector += user_vector\n",
    "    \n",
    "    # Normalize the group vector\n",
    "    norm = np.linalg.norm(group_vector)\n",
    "    if norm > 0:\n",
    "        group_vector /= norm\n",
    "    \n",
    "    return group_vector\n",
    "\n",
    "####\n",
    "#internal function, no api needed, just reads from database\n",
    "###\n",
    "def group_based_recommend(user_ids, n=10):\n",
    "    group_profile = get_group_profile(user_ids, tfidf, coordinate_scaler)\n",
    "    scores = cosine_similarity([group_profile], features)[0]\n",
    "    \n",
    "    top_indices = scores.argsort()[-n:][::-1]\n",
    "    recommended_ids = df.iloc[top_indices]['id'].tolist()\n",
    "    \n",
    "    return spot_details[spot_details['id'].isin(recommended_ids)][['id', 'name', 'image_url', 'phone']]\n",
    "\n",
    "####\n",
    "#create an api for this, read from database, and from user and display in ui (put to front end)\n",
    "###\n",
    "def get_next_group_spot(user_ids):\n",
    "    recommendations = group_based_recommend(user_ids, n=20)  # Get more recommendations for groups\n",
    "    \n",
    "    # Get all seen spots for the group\n",
    "    seen_spots = set()\n",
    "    for user_id in user_ids:\n",
    "        seen_spots.update(user_preferences[user_id]['location_specific'].keys())\n",
    "    \n",
    "    for _, spot in recommendations.iterrows():\n",
    "        if spot['id'] not in seen_spots:\n",
    "            return spot\n",
    "\n",
    "####\n",
    "#internal function, no api needed, just reads from database\n",
    "###\n",
    "def least_misery_group_recommend(user_ids, n=10):\n",
    "    individual_scores = []\n",
    "    for user_id in user_ids:\n",
    "        user_profile = get_user_profile(user_id, tfidf, coordinate_scaler)\n",
    "        scores = cosine_similarity([user_profile], features)[0]\n",
    "        individual_scores.append(scores)\n",
    "    \n",
    "    # Take the minimum score for each item across all users\n",
    "    group_scores = np.min(individual_scores, axis=0)\n",
    "    \n",
    "    top_indices = group_scores.argsort()[-n:][::-1]\n",
    "    recommended_ids = df.iloc[top_indices]['id'].tolist()\n",
    "    \n",
    "    return spot_details[spot_details['id'].isin(recommended_ids)][['id', 'name', 'image_url', 'phone']]\n",
    "\n",
    "    return None  # Return None if all recommended spots have been seen\n",
    "\n",
    "####\n",
    "#internal function, no api needed, just reads from database\n",
    "###\n",
    "def get_group_recommendation(user_ids):\n",
    "    # You could alternate between different group recommendation strategies\n",
    "    strategies = [group_based_recommend, least_misery_group_recommend]\n",
    "    strategy = random.choice(strategies)\n",
    "    \n",
    "    recommendations = strategy(user_ids, n=1)\n",
    "    if not recommendations.empty:\n",
    "        return recommendations.iloc[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#merely for testing purposes\n",
    "def main():\n",
    "    # add_new_user(1, {'price': '$$', 'categories': ['Italian', 'Bars'], 'coordinates': (39.9526, 75.1652)})\n",
    "    # update_user_preferences(1, {'categories': ['Italian', 'Bars', 'Seafood']})\n",
    "    # record_spot_interaction(1, 'j1S3NUrkB3BVT49n_e76NQ', {'time_viewing': 5.0})\n",
    "    # record_spot_interaction(1, 'zj8Lq1T8KIC5zwFief15jg', {'time_viewing': 2.0})\n",
    "\n",
    "    # next_spot = get_next_spot(1)\n",
    "    # print(\"Next spot to show:\", next_spot)\n",
    "\n",
    "    #stats = get_user_stats(1)\n",
    "    #print(\"User stats:\", stats)\n",
    "    #Add more test users\n",
    "    add_new_user(2, {'price': '$', 'categories': ['New American', 'Bars'], 'coordinates': (40.7128, 74.0060)}, 'Jack Ingof', 'password', 23)\n",
    "    add_new_user(3, {'price': '$$', 'categories': ['Mexican', 'Bars'], 'coordinates': (34.0522, 118.2437)},'Don T. Beakunt', 'peep', 11)\n",
    "    add_new_user(4, {'price': '$$$', 'categories': ['Japanese', 'Cafes'], 'coordinates': (35.6895, 139.6917)},'Ben Dover', 'bread6', 76)\n",
    "    add_new_user(5, {'price': '$', 'categories': ['Fish & Chips', 'Food Stands'], 'coordinates': (51.5074, 0.1278)},'Mike Hawk', 'mandilj', 999)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongodb+srv://loko:melike2004@lovelores.h1nkog2.mongodb.net/?retryWrites=true&w=majority&appName=LoveLores')\n",
    "db = client.GoSpot\n",
    "collection = db['User']\n",
    "#list every user in the database\n",
    "# list(collection.find({}))\n",
    "#collection.insert_one(user_preferences[1])\n",
    "# for key in user_preferences:\n",
    "#     user_preferences[key]['_id'] = key\n",
    "#     collection.insert_one(user_preferences[key])\n",
    "# list(collection.find({'_id':3}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['j1S3NUrkB3BVT49n_e76NQ', 'zj8Lq1T8KIC5zwFief15jg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(collection.find_one({'_id': 1}).get('location_specific',{}).keys()  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "example_df = pd.DataFrame([{'id': 1, 'name': 'Spot 1', 'image_url': 'url1', 'phone': '123-456-7890'},{ 'id': 2, 'name': 'Spot 2', 'image_url': 'url2', 'phone': '123-456-7890'}])\n",
    "#display each entry in the dataframe as a series\n",
    "for index, row in example_df.iterrows():\n",
    "    print(row['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult({'n': 9, 'electionId': ObjectId('7fffffff0000000000000022'), 'opTime': {'ts': Timestamp(1722787185, 11), 't': 34}, 'nModified': 9, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1722787185, 11), 'signature': {'hash': b'\\x04\\xc5\\x06\\x0c\\xa8\\xee\\r\\x81\\x02)\\xf2\\x81\\x12A\\xb9\\x80L\\x04u\\x1b', 'keyId': 7351819825506680836}}, 'operationTime': Timestamp(1722787185, 11), 'updatedExisting': True}, acknowledged=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = db['User']\n",
    "#create a new field 'groups' for each user, and set it to an empty list\n",
    "collection.update_many({},{'$set':{'groups':[]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.042782440211265205"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.rand() - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_annoy_index(features, n_trees=30):\\n    print(f\"Building Annoy index with {features.shape[1]} dimensions...\")\\n    f = features.shape[1]\\n    t = AnnoyIndex(f, \\'angular\\')\\n    try:\\n        for i in range(features.shape[0]):\\n            print(f\"Adding item {i} to the index\")\\n            t.add_item(i, features[i])\\n        print(f\"Added {features.shape[0]} items to the index\")\\n        \\n        print(f\"Starting to build index with {n_trees} trees...\")\\n        t.build(n_trees)\\n        print(f\"Built index with {n_trees} trees\")\\n    except Exception as e:\\n        print(f\"Error in build_annoy_index: {str(e)}\")\\n        import traceback\\n        traceback.print_exc()\\n        raise\\n    return t\\n\\ndef create_and_save_annoy_index(df):\\n    # print(\"Starting create_feature_matrix...\")\\n    features, tfidf, scaler = create_feature_matrix(df)\\n    # print(f\"Feature matrix created. Shape: {features.shape}\")\\n    \\n    # print(\"Building Annoy index...\")\\n    annoy_index = build_annoy_index(features)\\n    # print(\"Annoy index built successfully\")\\n    \\n    print(\"Saving Annoy index...\")\\n    try:\\n        annoy_index.save(\\'item_similarity.ann\\')\\n        print(\"Annoy index saved successfully\")\\n    except Exception as e:\\n        print(f\"Error saving Annoy index: {str(e)}\")\\n        # Try saving to a different location\\n        try:\\n            annoy_index.save(\\'C:/temp/item_similarity.ann\\')\\n            print(\"Annoy index saved successfully to C:/temp/\")\\n        except Exception as e:\\n            print(f\"Error saving Annoy index to C:/temp/: {str(e)}\")\\n    \\n    return features, tfidf, scaler\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, json, numpy as np, random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "category_mapping = {\n",
    "    'American': ['American', 'New American', 'Southern', 'Soul Food', 'Cajun/Creole', 'Tex-Mex'],\n",
    "    'Asian': ['Chinese', 'Japanese', 'Korean', 'Thai', 'Vietnamese', 'Indian', 'Pakistani', 'Bangladeshi', 'Taiwanese', 'Filipino', 'Malaysian', 'Indonesian', 'Singaporean', 'Burmese', 'Cambodian', 'Laotian', 'Mongolian', 'Nepalese', 'Sri Lankan', 'Asian Fusion'],\n",
    "    'European': ['Italian', 'French', 'Spanish', 'German', 'Greek', 'British', 'Irish', 'Scottish', 'Polish', 'Russian', 'Ukrainian', 'Hungarian', 'Czech', 'Austrian', 'Belgian', 'Dutch', 'Swiss', 'Scandinavian', 'Portuguese'],\n",
    "    'Latin American': ['Mexican', 'Brazilian', 'Peruvian', 'Argentine', 'Colombian', 'Venezuelan', 'Cuban', 'Puerto Rican', 'Dominican', 'Salvadoran', 'Honduran', 'Nicaraguan', 'Guatemalan', 'Ecuadorian', 'Bolivian', 'Chilean'],\n",
    "    'Middle Eastern': ['Lebanese', 'Turkish', 'Persian/Iranian', 'Israeli', 'Moroccan', 'Egyptian', 'Syrian', 'Armenian', 'Afghan', 'Iraqi', 'Uzbek', 'Georgian'],\n",
    "    'African': ['Ethiopian', 'Nigerian', 'Ghanaian', 'Senegalese', 'South African', 'Eritrean', 'Somali', 'Kenyan', 'Tanzanian', 'Ugandan'],\n",
    "    'Seafood': ['Seafood', 'Sushi Bars', 'Fish & Chips', 'Poke'],\n",
    "    'Fast Food': ['Fast Food', 'Burgers', 'Pizza', 'Sandwiches', 'Hot Dogs', 'Chicken Wings'],\n",
    "    'Vegetarian and Vegan': ['Vegetarian', 'Vegan', 'Raw Food'],\n",
    "    'Breakfast and Brunch': ['Breakfast & Brunch', 'Pancakes', 'Waffles', 'Bagels', 'Donuts'],\n",
    "    'Bakeries and Desserts': ['Bakeries', 'Desserts', 'Ice Cream & Frozen Yogurt', 'Cupcakes', 'Patisserie/Cake Shop', 'Gelato'],\n",
    "    'Cafes and Coffee Shops': ['Cafes', 'Coffee & Tea', 'Bubble Tea'],\n",
    "    'Bars and Pubs': ['Bars', 'Pubs', 'Sports Bars', 'Wine Bars', 'Beer Gardens', 'Cocktail Bars', 'Dive Bars', 'Hookah Bars'],\n",
    "    'Specialty Food': ['Cheese Shops', 'Butcher', 'Farmers Market', 'Specialty Food', 'Organic Stores', 'Health Markets'],\n",
    "    'Food Trucks and Stands': ['Food Trucks', 'Food Stands', 'Street Vendors'],\n",
    "    'Grocery': ['Grocery', 'International Grocery', 'Convenience Stores'],\n",
    "    'Nightlife': ['Nightlife', 'Dance Clubs', 'Karaoke', 'Comedy Clubs', 'Jazz & Blues'],\n",
    "    'Arts and Entertainment': ['Museums', 'Art Galleries', 'Performing Arts', 'Music Venues', 'Theaters', 'Cinema'],\n",
    "    'Outdoor Activities': ['Parks', 'Beaches', 'Hiking', 'Botanical Gardens', 'Playgrounds', 'Dog Parks'],\n",
    "    'Fitness and Sports': ['Gyms', 'Yoga', 'Martial Arts', 'Swimming Pools', 'Tennis', 'Basketball Courts', 'Soccer'],\n",
    "    'Shopping': ['Shopping Centers', 'Clothing', 'Shoes', 'Jewelry', 'Books', 'Electronics', 'Home & Garden'],\n",
    "    'Beauty and Spas': ['Hair Salons', 'Nail Salons', 'Day Spas', 'Massage'],\n",
    "    'Hotels and Accommodation': ['Hotels', 'Hostels', 'Bed & Breakfast'],\n",
    "    'Event Planning and Services': ['Wedding Planning', 'Party & Event Planning', 'Caterers', 'Photographers'],\n",
    "    'Automotive': ['Car Dealers', 'Auto Repair', 'Car Wash', 'Gas Stations'],\n",
    "    'Professional Services': ['Lawyers', 'Accountants', 'Real Estate', 'Insurance'],\n",
    "    'Education': ['Schools', 'Colleges', 'Tutoring', 'Cooking Classes', 'Art Schools'],\n",
    "    'Pets': ['Pet Stores', 'Veterinarians', 'Pet Groomers', 'Dog Walkers'],\n",
    "    'Religious Organizations': ['Churches', 'Mosques', 'Synagogues', 'Temples'],\n",
    "    'Other': []  # Catch-all for categories that don't fit elsewhere\n",
    "}\n",
    "\n",
    "# Create a reverse mapping for easy lookup\n",
    "reverse_category_mapping = {sub_cat: main_cat for main_cat, sub_cats in category_mapping.items() for sub_cat in sub_cats}\n",
    "\n",
    "with open('Updating_datasets/initial_weights.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "index=[]\n",
    "vals = []\n",
    "for key,value in data.items():\n",
    "    index.append(key)\n",
    "    vals.append([val for val in value.values()])\n",
    "    \n",
    "initial_weights = pd.DataFrame(vals,columns=[key for key in data[index[0]].keys()],index=index)\n",
    "\n",
    "def preprocess_data(initial_weights):\n",
    "    df = initial_weights.copy().reset_index()\n",
    "    df.rename(columns={'index': 'id'}, inplace=True)\n",
    "    \n",
    "    # Extract categories and map to general categories\n",
    "    df['categories'] = df['categories'].apply(lambda x: [reverse_category_mapping.get(cat['title'], 'Other') for cat in x])\n",
    "    df['categories'] = df['categories'].apply(lambda x: ', '.join(set(x)) if x else 'Other')\n",
    "    \n",
    "    # Handle missing values\n",
    "    df['review_count'] = df['review_count'].fillna(0)\n",
    "    df['rating'] = df['rating'].fillna(0)\n",
    "    df['price'] = df['price'].fillna('$')\n",
    "    df['latitude'] = df['coordinates'].apply(lambda x: x['latitude'] if x and 'latitude' in x else 0)\n",
    "    df['longitude'] = df['coordinates'].apply(lambda x: x['longitude'] if x and 'longitude' in x else 0)\n",
    "    df = df.drop(columns=['coordinates'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df = preprocess_data(initial_weights)\n",
    "#print(df)\n",
    "\n",
    "#one time thing, should be saved into the database as read only, unless we expand with more datasets\n",
    "def create_feature_matrix(df):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(df['categories'])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    numerical_features = scaler.fit_transform(df[['review_count', 'rating', 'latitude', 'longitude']].fillna(0))\n",
    "    \n",
    "    price_dummies = pd.get_dummies(df['price'], prefix='price').fillna(0)\n",
    "    \n",
    "    features = np.hstack((tfidf_matrix.toarray(), numerical_features, price_dummies.values))\n",
    "    \n",
    "    return features, tfidf, scaler\n",
    "\n",
    "def build_annoy_index(features, n_trees=150):\n",
    "    print(f\"Building Annoy index with {features.shape[1]} dimensions...\")\n",
    "    f = features.shape[1]\n",
    "    t = AnnoyIndex(f, 'angular')\n",
    "    try:\n",
    "        for i in range(features.shape[0]):\n",
    "            print(f\"Adding item {i} to the index\")\n",
    "            t.add_item(i, features[i])\n",
    "        print(f\"Added {features.shape[0]} items to the index\")\n",
    "        \n",
    "        print(f\"Starting to build index with {n_trees} trees...\")\n",
    "        t.build(n_trees)\n",
    "        print(f\"Built index with {n_trees} trees\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in build_annoy_index: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    return t\n",
    "\n",
    "def create_and_save_annoy_index(df):\n",
    "    # print(\"Starting create_feature_matrix...\")\n",
    "    features, tfidf, scaler = create_feature_matrix(df)\n",
    "    # print(f\"Feature matrix created. Shape: {features.shape}\")\n",
    "    \n",
    "    # print(\"Building Annoy index...\")\n",
    "    annoy_index = build_annoy_index(features)\n",
    "    # print(\"Annoy index built successfully\")\n",
    "    \n",
    "    print(\"Saving Annoy index...\")\n",
    "    try:\n",
    "        annoy_index.save('item_similarity.ann')\n",
    "        print(\"Annoy index saved successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Annoy index: {str(e)}\")\n",
    "        # Try saving to a different location\n",
    "        try:\n",
    "            annoy_index.save('C:/temp/item_similarity.ann')\n",
    "            print(\"Annoy index saved successfully to C:/temp/\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving Annoy index to C:/temp/: {str(e)}\")\n",
    "    \n",
    "    return features, tfidf, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,_,_ = create_feature_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb+srv://loko:melike2004@lovelores.h1nkog2.mongodb.net/?retryWrites=true&w=majority&appName=LoveLores')\n",
    "db = client.GoSpot\n",
    "collection = db['Spot']\n",
    "import json\n",
    "spots = json.load(open('../datasets/data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertOneResult('zj8Lq1T8KIC5zwFief15jg', acknowledged=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.insert_one(spots['businesses'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
